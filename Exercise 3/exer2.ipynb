{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Χρησιμοποιήθηκαν 2 free pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jimmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jimmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim \n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "revDf = pd.read_csv('reviewsRest.csv')\n",
    "restF = pd.read_csv('philly_restaurants_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                 business_id  category\n",
       "219  -AbzJTLQdbdQrhRzQLgsKA  Japanese\n",
       "400  -HxLFWKVgXSU8JlR21PBkg   Italian\n",
       "23   -LmhsdQproqCf5EQoD06rQ  Japanese\n",
       "154  -MkngKKkTIVfnUbq2S1ucQ   Italian\n",
       "233  -PMXnNJ1D67NkAupRHNkpQ   Italian\n",
       "..                      ...       ...\n",
       "39   zUJMvjK6aBeQtVCowZ85-w  Japanese\n",
       "240  zeounyPVXFZEz1c9KtptLA   Italian\n",
       "804  zgX8sYCRGVJ9M5LETpJ60A   Burgers\n",
       "401  zqisPpgCURrgLf4TVnI8RQ  Japanese\n",
       "55   zzyx5x0Z7xXWWvWnZFuxlQ   Italian\n",
       "\n",
       "[951 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restF.sort_values(by=['business_id'], inplace=True)\n",
    "restF.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Αυτήν την στιγμή έχουμε ένα DataFrame με κάθε business_id και το review του. Κάνουμε groupby με τα business_id, κρατάμε σε μια λίστα τα business_ids και δημιουργούμε μια νέα στήλη στην οποία έχουμε κάνει join κάθε review της επιχείρησης. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_groups = revDf.groupby('business_id')\n",
    "business_names = []\n",
    "text_joined = []\n",
    "\n",
    "for business_id, bus_data in bus_groups:\n",
    "    #print(business_id)\n",
    "    business_names.append(business_id)\n",
    "    #print(bus_data['text'])\n",
    "    text_joined.append(' '.join(bus_data[\"text\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Δημιουργούμε ένα DataFrame bus_reviews με δύο στήλες, στην πρώτη το business_id και στην άλλη όλα τα review ενωμένα. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                 business_id                                               text\n",
       "0    -AbzJTLQdbdQrhRzQLgsKA  Kingyo has summer appetizer specials for $3.00...\n",
       "1    -HxLFWKVgXSU8JlR21PBkg  \"Carluccio's: Way Delish\"\\n\\nEver find yoursel...\n",
       "2    -LmhsdQproqCf5EQoD06rQ  Down in south Philly for a meeting at my daugh...\n",
       "3    -MkngKKkTIVfnUbq2S1ucQ  Dee-frickin'-licious thick crust pizza!!! I or...\n",
       "4    -PMXnNJ1D67NkAupRHNkpQ  I've always preferred domino's over pizza hut!...\n",
       "..                      ...                                                ...\n",
       "946  zUJMvjK6aBeQtVCowZ85-w  Excelent! Food is wondeful, hot, and fresh! Th...\n",
       "947  zeounyPVXFZEz1c9KtptLA  Pizza, wings, steaks are good.. chicken finger...\n",
       "948  zgX8sYCRGVJ9M5LETpJ60A  I popped in there one afternoon to pick up som...\n",
       "949  zqisPpgCURrgLf4TVnI8RQ  This review is for poke bowl. Very fresh salad...\n",
       "950  zzyx5x0Z7xXWWvWnZFuxlQ  Maybe the pizza is good here... but I can real...\n",
       "\n",
       "[951 rows x 2 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_reviews = pd.DataFrame([])\n",
    "bus_reviews['business_id'] = pd.Series(business_names)\n",
    "bus_reviews['text'] = pd.Series(text_joined)\n",
    "bus_reviews.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Τώρα για να αναθέσουμε το category σε κάθε business_id ταξινομούμε το restF DataFrame και θέτουμε στο bus_review ως νέα στήλη την στήλη category_label από το restF. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                 business_id  \\\n",
       "0    -AbzJTLQdbdQrhRzQLgsKA   \n",
       "1    -HxLFWKVgXSU8JlR21PBkg   \n",
       "2    -LmhsdQproqCf5EQoD06rQ   \n",
       "3    -MkngKKkTIVfnUbq2S1ucQ   \n",
       "4    -PMXnNJ1D67NkAupRHNkpQ   \n",
       "..                      ...   \n",
       "946  zUJMvjK6aBeQtVCowZ85-w   \n",
       "947  zeounyPVXFZEz1c9KtptLA   \n",
       "948  zgX8sYCRGVJ9M5LETpJ60A   \n",
       "949  zqisPpgCURrgLf4TVnI8RQ   \n",
       "950  zzyx5x0Z7xXWWvWnZFuxlQ   \n",
       "\n",
       "                                                  text  category  \n",
       "0    Kingyo has summer appetizer specials for $3.00...  Japanese  \n",
       "1    \"Carluccio's: Way Delish\"\\n\\nEver find yoursel...   Italian  \n",
       "2    Down in south Philly for a meeting at my daugh...  Japanese  \n",
       "3    Dee-frickin'-licious thick crust pizza!!! I or...   Italian  \n",
       "4    I've always preferred domino's over pizza hut!...   Italian  \n",
       "..                                                 ...       ...  \n",
       "946  Excelent! Food is wondeful, hot, and fresh! Th...  Japanese  \n",
       "947  Pizza, wings, steaks are good.. chicken finger...   Italian  \n",
       "948  I popped in there one afternoon to pick up som...   Burgers  \n",
       "949  This review is for poke bowl. Very fresh salad...  Japanese  \n",
       "950  Maybe the pizza is good here... but I can real...   Italian  \n",
       "\n",
       "[951 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restF.sort_values(by=['business_id'], inplace=True)\n",
    "bus_reviews['category'] = restF['category'].tolist()\n",
    "bus_reviews.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Τώρα με την χρήση lambda function και την λίστα από τα stopwords που κατεβάσαμε στην αρχή, αν μια λέξη δεν περιέχεται στο stopwords list, την κρατάμε. Το νέο κείμενο που δημιουργείται, κρατιέται σε μια νέα στήλη. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                 business_id  \\\n",
       "0    -AbzJTLQdbdQrhRzQLgsKA   \n",
       "1    -HxLFWKVgXSU8JlR21PBkg   \n",
       "2    -LmhsdQproqCf5EQoD06rQ   \n",
       "3    -MkngKKkTIVfnUbq2S1ucQ   \n",
       "4    -PMXnNJ1D67NkAupRHNkpQ   \n",
       "..                      ...   \n",
       "946  zUJMvjK6aBeQtVCowZ85-w   \n",
       "947  zeounyPVXFZEz1c9KtptLA   \n",
       "948  zgX8sYCRGVJ9M5LETpJ60A   \n",
       "949  zqisPpgCURrgLf4TVnI8RQ   \n",
       "950  zzyx5x0Z7xXWWvWnZFuxlQ   \n",
       "\n",
       "                                                  text  category  \\\n",
       "0    Kingyo has summer appetizer specials for $3.00...  Japanese   \n",
       "1    \"Carluccio's: Way Delish\"\\n\\nEver find yoursel...   Italian   \n",
       "2    Down in south Philly for a meeting at my daugh...  Japanese   \n",
       "3    Dee-frickin'-licious thick crust pizza!!! I or...   Italian   \n",
       "4    I've always preferred domino's over pizza hut!...   Italian   \n",
       "..                                                 ...       ...   \n",
       "946  Excelent! Food is wondeful, hot, and fresh! Th...  Japanese   \n",
       "947  Pizza, wings, steaks are good.. chicken finger...   Italian   \n",
       "948  I popped in there one afternoon to pick up som...   Burgers   \n",
       "949  This review is for poke bowl. Very fresh salad...  Japanese   \n",
       "950  Maybe the pizza is good here... but I can real...   Italian   \n",
       "\n",
       "                                       text_without_sw  \n",
       "0    Kingyo summer appetizer specials $3.00 Yes - t...  \n",
       "1    \"Carluccio's: Way Delish\" Ever find captivated...  \n",
       "2    Down south Philly meeting daughter's school, l...  \n",
       "3    Dee-frickin'-licious thick crust pizza!!! I or...  \n",
       "4    I've always preferred domino's pizza hut! I or...  \n",
       "..                                                 ...  \n",
       "946  Excelent! Food wondeful, hot, fresh! The sushi...  \n",
       "947  Pizza, wings, steaks good.. chicken fingers ba...  \n",
       "948  I popped one afternoon pick lunch take-out. It...  \n",
       "949  This review poke bowl. Very fresh salad fish. ...  \n",
       "950  Maybe pizza good here... I really speak soup s...  \n",
       "\n",
       "[951 rows x 4 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "bus_reviews['text_without_sw'] = bus_reviews['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "bus_reviews.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(760, 4) (191, 4)\n",
      "(761, 4) (190, 4)\n",
      "(761, 4) (190, 4)\n",
      "(761, 4) (190, 4)\n",
      "(761, 4) (190, 4)\n"
     ]
    }
   ],
   "source": [
    "bus_reviews = shuffle(bus_reviews)\n",
    "folds = KFold(n_splits=5)\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "for trainIndex, testIndex in folds.split(bus_reviews):\n",
    "    # print(trainIndex, testIndex)\n",
    "    xTrain = bus_reviews.loc[trainIndex, :]\n",
    "    xTest = bus_reviews.loc[testIndex, :]\n",
    "    train_folds.append(xTrain)\n",
    "    test_folds.append(xTest)\n",
    "    print(xTrain.shape, xTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Τώρα για κάθε business_id έχουμε την ένωση όλων των reviews χωρίς stopwords, και για αυτή την ένωση κειμένων βγάζουμε σε ένα array τα αποτελέσματα του tfid αλγορίθμου. Μετά, κρατάμε στο feature_names τις λέξεις του κειμένου που θα χρησιμοποιηθούν αργότερα. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# VERSION 2 #######################################\n",
    "train_set = []\n",
    "test_set = []\n",
    "for i in range(len(train_folds)):\n",
    "    v = TfidfVectorizer(stop_words = 'english', min_df = 0.2, max_df = 0.8, max_features=3000)\n",
    "    #print(\"aa\")\n",
    "    x = v.fit_transform(train_folds[i]['text_without_sw']).toarray()\n",
    "    train_set.append(x)\n",
    "    y = v.transform(test_folds[i]['text_without_sw']).toarray()\n",
    "    test_set.append(y)\n",
    "#x = v.fit_transform(bus_reviews['text_without_sw']).toarray()\n",
    "\n",
    "feature_names = v.get_feature_names()\n",
    "############################# VERSION 2 #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 2011\n",
      "1989 1989\n",
      "2084 2084\n",
      "2012 2012\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set[0][0]),len(test_set[0][0]))\n",
    "print(len(train_set[1][0]),len(test_set[1][0]))\n",
    "print(len(train_set[2][0]),len(test_set[2][0]))\n",
    "print(len(train_set[3][0]),len(test_set[3][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oi grammes einai oi pragmatikes times\n",
    "# 1h grammh: 316 ta burgers\n",
    "# 2h grammh: 431 ta italika\n",
    "# 3h grammh: 204 ta iapwnika"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Logistic </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56  5  2]\n",
      " [ 7 80  0]\n",
      " [ 0  0 38]]\n",
      "Accuracy:   0.9179608707632957\n",
      "Recall:    [0.88651472 0.91335592 0.983467  ]\n",
      "Precision: [0.88410181 0.93380337 0.93068206]\n",
      "F1_Score:  [0.88487407 0.92310786 0.95603393]\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = LogisticRegression(random_state=16)\n",
    "    clf.fit(train_set[i],train_folds[i]['category'])\n",
    "    y_pred = clf.predict(test_set[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, test_folds[i]['category']))\n",
    "    prec.append(metrics.precision_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, test_folds[i]['category'])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> SVM </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55  5  1]\n",
      " [ 6 80  1]\n",
      " [ 0  0 38]]\n",
      "Accuracy:   0.9137558556076053\n",
      "Recall:    [0.88198532 0.91224145 0.96923228]\n",
      "Precision: [0.87822351 0.92914406 0.93034946]\n",
      "F1_Score:  [0.87998192 0.9202957  0.94866767]\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "    clf.fit(train_set[i],train_folds[i]['category'])\n",
    "    y_pred = clf.predict(test_set[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, test_folds[i]['category']))\n",
    "    prec.append(metrics.precision_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, test_folds[i]['category'])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> k-NN </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52  6  1]\n",
      " [ 9 79  1]\n",
      " [ 0  0 38]]\n",
      "Accuracy:   0.8979774042435933\n",
      "Recall:    [0.87977727 0.88109859 0.96895527]\n",
      "Precision: [0.83247528 0.92208556 0.94136582]\n",
      "F1_Score:  [0.85462505 0.90043428 0.95436458]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = KNeighborsClassifier(n_neighbors = 20)\n",
    "    clf.fit(train_set[i],train_folds[i]['category'])\n",
    "    y_pred = clf.predict(test_set[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, test_folds[i]['category']))\n",
    "    prec.append(metrics.precision_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, test_folds[i]['category'], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, test_folds[i]['category'])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Χρησιμοποιούμε την έτοιμη συνάρτηση για k-fold logistic και κρατάμε τα βάρη σε ένα DataFrame, όπου κάθε γραμμή αντιστοιχεί και σε κάθε label (Burger, Italian, Japanese) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "logistic = LogisticRegression(random_state=16)\n",
    "logistic.fit(train_set[i],train_folds[i]['category'])\n",
    "w = logistic.coef_ #weights of each feature\n",
    "\n",
    "df = pd.DataFrame(w, columns=feature_names)\n",
    "\n",
    "# Change the row indexes\n",
    "df.index = ['0', '1', '2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Οι 10 πιο σημαντικές και οι πιο ξένες λέξεις για τα εστιατόρια Burgers </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST valuable Burgers restaurants words:\n",
      "fries          2.134281\n",
      "burger         2.103392\n",
      "burgers        1.229712\n",
      "drive          1.168020\n",
      "breakfast      1.092934\n",
      "wings          0.916595\n",
      "mac            0.832220\n",
      "cheesesteak    0.694895\n",
      "sandwich       0.670733\n",
      "window         0.569602\n",
      "dirty          0.524541\n",
      "bacon          0.516179\n",
      "dog            0.462561\n",
      "worst          0.455936\n",
      "horrible       0.451668\n",
      "waited         0.444631\n",
      "buffalo        0.437612\n",
      "beer           0.416832\n",
      "location       0.401911\n",
      "employees      0.400823\n",
      "late           0.390432\n",
      "Name: 0, dtype: float64\n",
      "########################################\n",
      "LEAST valuable Burgers restaurants words:\n",
      "sushi       -1.439078\n",
      "italian     -1.308981\n",
      "pasta       -1.156885\n",
      "pork        -1.037388\n",
      "rice        -0.892452\n",
      "wine        -0.806721\n",
      "japanese    -0.728176\n",
      "pizza       -0.723810\n",
      "spicy       -0.591858\n",
      "meatballs   -0.585062\n",
      "dinner      -0.566197\n",
      "noodles     -0.559680\n",
      "broth       -0.558542\n",
      "dishes      -0.541073\n",
      "calamari    -0.531610\n",
      "bowl        -0.514072\n",
      "dumplings   -0.511178\n",
      "soup        -0.508118\n",
      "owner       -0.501427\n",
      "byob        -0.476074\n",
      "club        -0.463857\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rslt_df = df.sort_values(by = '0', axis = 1, ascending = False)\n",
    "rslt_ll = rslt_df.iloc[0]\n",
    "print(\"MOST valuable Burgers restaurants words:\")\n",
    "print(rslt_ll[:21])\n",
    "print(\"########################################\")\n",
    "rslt_df = df.sort_values(by = '0', axis = 1, ascending = True)\n",
    "rslt_ll = rslt_df.iloc[0]\n",
    "print(\"LEAST valuable Burgers restaurants words:\")\n",
    "print(rslt_ll[:21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Οι 10 πιο σημαντικές και οι πιο ξένες λέξεις για τα εστιατόρια Italian </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST valuable Italian restaurants words:\n",
      "pizza        -0.723810\n",
      "italian      -1.308981\n",
      "pasta        -1.156885\n",
      "wine         -0.806721\n",
      "bread        -0.428513\n",
      "meatballs    -0.585062\n",
      "dinner       -0.566197\n",
      "hoagie       -0.101680\n",
      "coffee       -0.264474\n",
      "gnocchi      -0.390034\n",
      "salad        -0.455406\n",
      "byob         -0.476074\n",
      "club         -0.463857\n",
      "parm         -0.373843\n",
      "calamari     -0.531610\n",
      "veal         -0.323491\n",
      "mozzarella   -0.391329\n",
      "meatball     -0.338559\n",
      "sandwiches    0.087374\n",
      "ravioli      -0.253432\n",
      "crust        -0.255564\n",
      "Name: 0, dtype: float64\n",
      "########################################\n",
      "LEAST valuable Italian restaurants words:\n",
      "sushi       -1.439078\n",
      "fries        2.134281\n",
      "burger       2.103392\n",
      "burgers      1.229712\n",
      "drive        1.168020\n",
      "noodles     -0.559680\n",
      "broth       -0.558542\n",
      "chinese     -0.408421\n",
      "bowl        -0.514072\n",
      "japanese    -0.728176\n",
      "ice          0.147607\n",
      "rice        -0.892452\n",
      "bbq          0.070365\n",
      "wings        0.916595\n",
      "mac          0.832220\n",
      "dumplings   -0.511178\n",
      "salmon       0.109569\n",
      "dirty        0.524541\n",
      "shrimp      -0.012936\n",
      "tea         -0.436330\n",
      "platter      0.350980\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rslt_df = df.sort_values(by = '1', axis = 1, ascending = False)\n",
    "rslt_ll = rslt_df.iloc[0]\n",
    "print(\"MOST valuable Italian restaurants words:\")\n",
    "print(rslt_ll[:21])\n",
    "print(\"########################################\")\n",
    "rslt_df = df.sort_values(by = '1', axis = 1, ascending = True)\n",
    "rslt_ll = rslt_df.iloc[0]\n",
    "print(\"LEAST valuable Italian restaurants words:\")\n",
    "print(rslt_ll[:21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Οι 10 πιο σημαντικές και οι πιο ξένες λέξεις για τα εστιατόρια Japanese </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST valuable Japanese restaurants words:\n",
      "sushi       -1.439078\n",
      "rice        -0.892452\n",
      "japanese    -0.728176\n",
      "noodles     -0.559680\n",
      "broth       -0.558542\n",
      "bowl        -0.514072\n",
      "chinese     -0.408421\n",
      "pork        -1.037388\n",
      "dumplings   -0.511178\n",
      "spicy       -0.591858\n",
      "tea         -0.436330\n",
      "soup        -0.508118\n",
      "rolls       -0.445268\n",
      "roll        -0.366842\n",
      "fried       -0.255254\n",
      "beef        -0.222559\n",
      "tuna        -0.308703\n",
      "bowls       -0.271210\n",
      "room        -0.440289\n",
      "noodle      -0.198795\n",
      "shrimp      -0.012936\n",
      "Name: 0, dtype: float64\n",
      "########################################\n",
      "LEAST valuable Japanese restaurants words:\n",
      "pizza         -0.723810\n",
      "burger         2.103392\n",
      "sandwich       0.670733\n",
      "italian       -1.308981\n",
      "breakfast      1.092934\n",
      "fries          2.134281\n",
      "pasta         -1.156885\n",
      "bread         -0.428513\n",
      "hoagie        -0.101680\n",
      "sandwiches     0.087374\n",
      "burgers        1.229712\n",
      "steak          0.211505\n",
      "wings          0.916595\n",
      "cheesesteak    0.694895\n",
      "brunch         0.194898\n",
      "drive          1.168020\n",
      "coffee        -0.264474\n",
      "beer           0.416832\n",
      "wine          -0.806721\n",
      "mac            0.832220\n",
      "bacon          0.516179\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rslt_df = df.sort_values(by = '2', axis = 1, ascending = False)\n",
    "rslt_ll = rslt_df.iloc[0]\n",
    "print(\"MOST valuable Japanese restaurants words:\")\n",
    "print(rslt_ll[:21])\n",
    "print(\"########################################\")\n",
    "rslt_df = df.sort_values(by = '2', axis = 1, ascending = True)\n",
    "rslt_ll = rslt_df.iloc[0]\n",
    "print(\"LEAST valuable Japanese restaurants words:\")\n",
    "print(rslt_ll[:21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN PARAMS\n",
    "# i: 0->4\n",
    "# train_folds[i],train_folds[i]['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gsim_L = []\n",
    "y_train_gsim_L = []\n",
    "for i in range(5):\n",
    "    train_gsim = [utils.simple_preprocess(x) for x in train_folds[i]['text']]\n",
    "    train_data_labels = [(x,y) for (x,y) in zip(train_gsim, train_folds[i]['category']) if len(x) > 0]\n",
    "    X_train_gsim = [x for (x,y) in train_data_labels]\n",
    "    y_train_gsim = [y for (x,y) in train_data_labels]\n",
    "    X_train_gsim_L.append(X_train_gsim)\n",
    "    y_train_gsim_L.append(y_train_gsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PARAMS\n",
    "# i: 0->4\n",
    "# test_folds[i],test_folds[i]['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_gsim_L = []\n",
    "y_test_gsim_L = []\n",
    "for i in range(5):\n",
    "    test_gsim = [utils.simple_preprocess(x) for x in test_folds[i]['text']]\n",
    "    test_data_labels = [(x,y) for (x,y) in zip(test_gsim, test_folds[i]['category']) if len(x) > 0]\n",
    "    X_test_gsim = [x for (x,y) in test_data_labels]\n",
    "    y_test_gsim = [y for (x,y) in test_data_labels]\n",
    "    X_test_gsim_L.append(X_test_gsim)\n",
    "    y_test_gsim_L.append(y_test_gsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760 760\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_gsim_L[0]),len(y_train_gsim_L[0]))\n",
    "print(len(X_test_gsim_L),len(y_test_gsim_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "cbow_list = []\n",
    "# sg=0: cbow, sg=1: skipgram\n",
    "for i in range(5):\n",
    "    cbow_model = gensim.models.Word2Vec(X_train_gsim_L[i], min_count = 1, window = 10, sg=0)\n",
    "    cbow_list.append(cbow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cbow = []\n",
    "for i in range(5):\n",
    "    X_train_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y]).mean(axis = 0) for y in X_train_gsim_L[i]]\n",
    "    X_train_cbow.append(X_train_cbow_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cbow = []\n",
    "for i in range(5):\n",
    "    X_test_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y if x in cbow_list[i].wv]).mean(axis = 0) for y in X_test_gsim_L[i]]\n",
    "    X_test_cbow.append(X_test_cbow_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Logistic </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55  6  0]\n",
      " [ 7 79  1]\n",
      " [ 0  0 39]]\n",
      "Accuracy:   0.9137558556076053\n",
      "Recall:    [0.88720992 0.90247666 0.98430324]\n",
      "Precision: [0.87451297 0.91461565 0.96567362]\n",
      "F1_Score:  [0.87995274 0.90815591 0.97471536]\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = LogisticRegression(random_state=16,solver='lbfgs', max_iter=200)\n",
    "    clf.fit(X_train_cbow[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(X_test_cbow[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> SVM </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56  7  0]\n",
      " [ 6 78  0]\n",
      " [ 0  0 39]]\n",
      "Accuracy:   0.9158611187655001\n",
      "Recall:    [0.88067963 0.91500587 0.97892603]\n",
      "Precision: [0.88750066 0.90979389 0.96534103]\n",
      "F1_Score:  [0.88309351 0.91178369 0.97193004]\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "    clf.fit(X_train_cbow[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(X_test_cbow[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> k-NN </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52  8  1]\n",
      " [10 77  1]\n",
      " [ 0  0 38]]\n",
      "Accuracy:   0.8801047120418849\n",
      "Recall:    [0.83774083 0.87062739 0.9688764 ]\n",
      "Precision: [0.82747898 0.89260321 0.93024239]\n",
      "F1_Score:  [0.83204988 0.88144216 0.94845997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = KNeighborsClassifier(n_neighbors = 20, metric='cosine')\n",
    "    clf.fit(X_train_cbow[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(X_test_cbow[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## MAYBE REPETITIVE CODE, WE HAD IT ALREADY IN CBOW\n",
    "# TRAIN PARAMS\n",
    "# i: 0->4\n",
    "# train_folds[i],train_folds[i]['category']\n",
    "\n",
    "X_train_gsim_L = []\n",
    "y_train_gsim_L = []\n",
    "for i in range(5):\n",
    "    train_gsim = [utils.simple_preprocess(x) for x in train_folds[i]['text']]\n",
    "    train_data_labels = [(x,y) for (x,y) in zip(train_gsim, train_folds[i]['category']) if len(x) > 0]\n",
    "    X_train_gsim = [x for (x,y) in train_data_labels]\n",
    "    y_train_gsim = [y for (x,y) in train_data_labels]\n",
    "    X_train_gsim_L.append(X_train_gsim)\n",
    "    y_train_gsim_L.append(y_train_gsim)\n",
    "\n",
    "# TEST PARAMS\n",
    "# i: 0->4\n",
    "# test_folds[i],test_folds[i]['category']\n",
    "\n",
    "X_test_gsim_L = []\n",
    "y_test_gsim_L = []\n",
    "for i in range(5):\n",
    "    test_gsim = [utils.simple_preprocess(x) for x in test_folds[i]['text']]\n",
    "    test_data_labels = [(x,y) for (x,y) in zip(test_gsim, test_folds[i]['category']) if len(x) > 0]\n",
    "    X_test_gsim = [x for (x,y) in test_data_labels]\n",
    "    y_test_gsim = [y for (x,y) in test_data_labels]\n",
    "    X_test_gsim_L.append(X_test_gsim)\n",
    "    y_test_gsim_L.append(y_test_gsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold\n",
      "Fold\n",
      "Fold\n",
      "Fold\n",
      "Fold\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "cbow_list = []\n",
    "# sg=0: cbow, sg=1: skipgram\n",
    "for i in range(5):\n",
    "    print(\"Fold\")\n",
    "    cbow_model = gensim.models.Word2Vec(X_train_gsim_L[i], min_count = 1, window = 10, sg=1)\n",
    "    cbow_list.append(cbow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cbow = []\n",
    "for i in range(5):\n",
    "    X_train_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y]).mean(axis = 0) for y in X_train_gsim_L[i]]\n",
    "    X_train_cbow.append(X_train_cbow_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cbow = []\n",
    "for i in range(5):\n",
    "    X_test_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y if x in cbow_list[i].wv]).mean(axis = 0) for y in X_test_gsim_L[i]]\n",
    "    X_test_cbow.append(X_test_cbow_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Logistic </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  6  1]\n",
      " [15 79  4]\n",
      " [ 0  0 35]]\n",
      "Accuracy:   0.8569633507853404\n",
      "Recall:    [0.86147268 0.80708035 0.98267974]\n",
      "Precision: [0.75764955 0.91859939 0.87784126]\n",
      "F1_Score:  [0.80598849 0.85873799 0.92612787]\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = LogisticRegression(random_state=16,solver='lbfgs', max_iter=200)\n",
    "    clf.fit(X_train_cbow[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(X_test_cbow[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> SVM </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  3  1]\n",
      " [12 82  2]\n",
      " [ 0  0 37]]\n",
      "Accuracy:   0.895888674565996\n",
      "Recall:    [0.9184314  0.85017245 0.97756673]\n",
      "Precision: [0.79531454 0.95822458 0.91560824]\n",
      "F1_Score:  [0.85212476 0.90076655 0.94532556]\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "    clf.fit(X_train_cbow[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(X_test_cbow[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52  6  2]\n",
      " [10 79  1]\n",
      " [ 0  0 37]]\n",
      "Accuracy:   0.8885147423532654\n",
      "Recall:    [0.86118768 0.86999667 0.97755153]\n",
      "Precision: [0.82055445 0.92203188 0.91535808]\n",
      "F1_Score:  [0.83974043 0.89491033 0.94488966]\n"
     ]
    }
   ],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = KNeighborsClassifier(n_neighbors = 20, metric='cosine')\n",
    "    clf.fit(X_train_cbow[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(X_test_cbow[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN PARAMS\n",
    "# i: 0->4\n",
    "# train_folds[i],train_folds[i]['category']\n",
    "\n",
    "X_train_gsim_L = []\n",
    "y_train_gsim_L = []\n",
    "for i in range(5):\n",
    "    train_gsim = [utils.simple_preprocess(x) for x in train_folds[i]['text']]\n",
    "    train_data_labels = [(x,y) for (x,y) in zip(train_gsim, train_folds[i]['category']) if len(x) > 0]\n",
    "    X_train_gsim = [x for (x,y) in train_data_labels]\n",
    "    y_train_gsim = [y for (x,y) in train_data_labels]\n",
    "    X_train_gsim_L.append(X_train_gsim)\n",
    "    y_train_gsim_L.append(y_train_gsim)\n",
    "\n",
    "# TEST PARAMS\n",
    "# i: 0->4\n",
    "# test_folds[i],test_folds[i]['category']\n",
    "\n",
    "X_test_gsim_L = []\n",
    "y_test_gsim_L = []\n",
    "for i in range(5):\n",
    "    test_gsim = [utils.simple_preprocess(x) for x in test_folds[i]['text']]\n",
    "    test_data_labels = [(x,y) for (x,y) in zip(test_gsim, test_folds[i]['category']) if len(x) > 0]\n",
    "    X_test_gsim = [x for (x,y) in test_data_labels]\n",
    "    y_test_gsim = [y for (x,y) in test_data_labels]\n",
    "    X_test_gsim_L.append(X_test_gsim)\n",
    "    y_test_gsim_L.append(y_test_gsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold\n",
      "760\n",
      "Fold\n",
      "761\n",
      "Fold\n",
      "761\n",
      "Fold\n",
      "761\n",
      "Fold\n",
      "761\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "d2v_Train = []\n",
    "d2v_Test = []\n",
    "# sg=0: cbow, sg=1: skipgram\n",
    "for j in range(5):\n",
    "    print(\"Fold\")\n",
    "    train_corpus = [gensim.models.doc2vec.TaggedDocument(X_train_gsim_L[j][i], [i]) for i in range(len(X_train_gsim_L[j]))]\n",
    "    d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    d2v_model.build_vocab(train_corpus)\n",
    "    d2v_model.train(train_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "    print(len(X_train_gsim_L[j]))\n",
    "    X_train_d2v = [d2v_model.infer_vector(x) for x in X_train_gsim_L[j]]\n",
    "    # follwoing line fixed!\n",
    "    X_test_d2v = [d2v_model.infer_vector(x) for x in X_test_gsim_L[j]]\n",
    "    d2v_Train.append(X_train_d2v)\n",
    "    d2v_Test.append(X_test_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cbow = []\n",
    "for i in range(5):\n",
    "    X_train_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y]).mean(axis = 0) for y in X_train_gsim_L[i]]\n",
    "    X_train_cbow.append(X_train_cbow_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-40-c0df0c646ac2>\", line 3, in <module>\n",
      "    X_test_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y if x in cbow_list[i].wv]).mean(axis = 0) for y in X_test_gsim_L[i]]\n",
      "  File \"<ipython-input-40-c0df0c646ac2>\", line 3, in <listcomp>\n",
      "    X_test_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y if x in cbow_list[i].wv]).mean(axis = 0) for y in X_test_gsim_L[i]]\n",
      "  File \"<ipython-input-40-c0df0c646ac2>\", line 3, in <listcomp>\n",
      "    X_test_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y if x in cbow_list[i].wv]).mean(axis = 0) for y in X_test_gsim_L[i]]\n",
      "  File \"D:\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\", line 649, in __contains__\n",
      "    return self.has_index_for(key)\n",
      "  File \"D:\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\", line 646, in has_index_for\n",
      "    return self.get_index(key, -1) >= 0\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"D:\\anaconda3\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"D:\\anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"D:\\anaconda3\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"D:\\anaconda3\\lib\\ntpath.py\", line 647, in realpath\n",
      "    path = _getfinalpathname(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "X_test_cbow = []\n",
    "for i in range(5):\n",
    "    X_test_cbow_temp = [np.array([cbow_list[i].wv[x] for x in y if x in cbow_list[i].wv]).mean(axis = 0) for y in X_test_gsim_L[i]]\n",
    "    X_test_cbow.append(X_test_cbow_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> Logistic </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = LogisticRegression(random_state=16,solver='lbfgs', max_iter=1000)\n",
    "    clf.fit(d2v_Train[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(d2v_Test[i])\n",
    "    #print(len(y_pred),len(y_test_gsim_L[i]),len(d2v_Test[i]))\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:700;font-size:18px\"> SVM </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "    clf.fit(X_train_cbow[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(X_test_cbow[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "for i in range(5):\n",
    "    clf = KNeighborsClassifier(n_neighbors = 20, metric='cosine')\n",
    "    clf.fit(X_train_cbow[i],y_train_gsim_L[i])\n",
    "    y_pred = clf.predict(X_test_cbow[i])\n",
    "    acc.append(metrics.accuracy_score(y_pred, y_test_gsim_L[i]))\n",
    "    prec.append(metrics.precision_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    rec.append(metrics.recall_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    f1.append(metrics.f1_score(y_pred, y_test_gsim_L[i], average=None))\n",
    "    conf_mat = confusion_matrix(y_pred, y_test_gsim_L[i])\n",
    "    conf = conf + conf_mat\n",
    "conf_mean = conf/5\n",
    "conf_mean = conf_mean.astype(int)\n",
    "rec = np.array(rec)\n",
    "rec = rec.mean(axis=0)\n",
    "\n",
    "prec = np.array(prec)\n",
    "prec = prec.mean(axis=0)\n",
    "\n",
    "f1 = np.array(f1)\n",
    "f1 = f1.mean(axis=0)\n",
    "\n",
    "acc = np.array(acc)\n",
    "print(conf_mean)\n",
    "print(\"Accuracy:  \",acc.mean())\n",
    "print(\"Recall:   \",rec)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"F1_Score: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
